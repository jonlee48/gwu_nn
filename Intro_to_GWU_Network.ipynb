{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding our Class Library\n",
    "We'll be using the gwu_nn library to dive a bit deeper into how neural networks work and get a better handle on how we can manipulate our layers and activation functions to build better networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gwu_nn.gwu_network import GWUNetwork\n",
    "from gwu_nn.layers import Dense\n",
    "from gwu_nn.activation_layers import Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our Data\n",
    "To explore how our GWU_Network library works we'll reuse the example data from the ML Crash Course. As the data is not linearly solvable, solving this problem will also show the flexibility of our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_col = 'Survived'\n",
    "x_cols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "df = pd.read_csv('titanic_data.csv')\n",
    "y = np.array(df[y_col]).reshape(-1, 1)\n",
    "orig_X = df[x_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Lets standardize our features\n",
    "scaler = preprocessing.StandardScaler()\n",
    "stand_X = scaler.fit_transform(orig_X)\n",
    "X = stand_X\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(478, 7)\n",
      "[ 0.91123237  0.75905134 -0.32371271 -0.55170307 -0.50589515 -0.65607592\n",
      " -0.50349899]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0].shape)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GWUNetwork Class\n",
    "This class is the main structure used to house our network. Like many standard libraries, our GWUNetwork or neural network model works by adding layers to it.\n",
    "\n",
    "```python\n",
    "class GWUNetwork():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    def add(self, layer):\n",
    "\n",
    "    def get_weights(self):\n",
    "\n",
    "    #def set_loss(self, loss):\n",
    "       \n",
    "    def predict(self, input_data):\n",
    "       \n",
    "    def measure_loss(self, x, y):\n",
    "\n",
    "    def fit(self, x_train, y_train, epochs):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we create a network it is initially empty, as it doesn't contain any layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Empty\n"
     ]
    }
   ],
   "source": [
    "network = GWUNetwork()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Layers\n",
    "To begin building our network we need to start adding functionality or layers. Each layer we add will increase the complexity and depth of our model. However, we'll find that for most simple problems 3-4 layers are typically enough.\n",
    "\n",
    "Currently the GWU_NN library only supports densely connected layers, this means that every input node is connected to every output node.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/iHW2o.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "Dense - (Input:7, Output:16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network.add(Dense(16, add_bias=True, activation='relu', input_size=7))\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Dense Layer\n",
    "```python\n",
    "class Dense:\n",
    "    def __init__(self, output_size, add_bias=False, activation=None, input_size=None):\n",
    "        super().__init__(activation)\n",
    "        self.type = None\n",
    "        self.name = \"Dense\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.add_bias = add_bias\n",
    "        \n",
    "    def forward_propagation(self, input):\n",
    "        self.input = input\n",
    "        output = np.dot(input, self.weights)\n",
    "        if self.add_bias:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        if self.add_bias:\n",
    "            self.bias -= learning_rate * output_error\n",
    "        return input_error\n",
    "```\n",
    "\n",
    "Notice that in our layer code, we actually define the forward and backward propogation steps. Due to the nature of how neural networks work (and the core principals of computational graphs), calculating forward/backward passes does not matter on the entire network structure.\n",
    "\n",
    "Forward Progpogation:\n",
    " - When calculating the forward propogation, we simply need to know the values coming into the layer. Thus our model simply passes the previous layer's output to the subsequent one\n",
    " \n",
    "Backwards Propogation:\n",
    " - This works similar to forward propogation but in reverse. The major difference here is that while the error is propogating backwards, the weights are being updated on each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Hidden Layer\n",
    "To take true advantage of our network, we need to add a hidden layer. For our GWU_NN models each layer defines the number of input and output nodes. So creating a hidden layer simply requires the addition of another layer.\n",
    "\n",
    "*Note: As we'll only have one hidden layer, we'll want our output to match the size/shape of our final prediction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "Dense - (Input:7, Output:16)\n",
      "Dense - (Input:16, Output:1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network.add(Dense(1, True))\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Activation Layers/Functions\n",
    "As previously discussed, activation layers help shape our network and let us interpret results for a given prediction/outcome. In this instance we will continue to use Sigmoid activation layer/function to predict binary classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "Dense - (Input:7, Output:16)\n",
      "Dense - (Input:16, Output:1)\n",
      "Sigmoid Activation\n"
     ]
    }
   ],
   "source": [
    "network.add(Sigmoid())\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Layer/Function\n",
    "\n",
    "Notice that the sigmoid layer/function works nearly identical to the Dense layer, just here:\n",
    " - activation -> forward propogation\n",
    " - activation_partial_derivative -> backward propogation\n",
    " \n",
    "``` python\n",
    "class SigmoidActivation(ActivationFunction):\n",
    "\n",
    "    @classmethod\n",
    "    def activation(cls, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @classmethod\n",
    "    def activation_partial_derivative(cls, x):\n",
    "        return np.exp(-x) / (1 + np.exp(-x))**2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Our Model\n",
    "Finally to complete our model we need to compile it. This final step defines our loss function, optimizer, learning_rate, and more (when working with larger libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "Dense - (Input:7, Output:16)\n",
      "Dense - (Input:16, Output:1)\n",
      "Sigmoid Activation\n"
     ]
    }
   ],
   "source": [
    "network.compile(loss='log_loss', lr=0.01)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Loss Function\n",
    "We can see that our **log_loss** function parallels exactly what we had in our perceptron/logistic regression computational graph\n",
    "\n",
    "```python\n",
    "class LogLoss(LossFunction):\n",
    "\n",
    "    @classmethod\n",
    "    def loss(cls, y_true, y_pred):\n",
    "        return np.mean(-np.log(y_pred)*y_true + -np.log(1-y_pred)*(1-y_true))\n",
    "\n",
    "    @classmethod\n",
    "    def loss_partial_derivative(cls, y_true, y_pred):\n",
    "        return -np.sum(y_true - y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting or Training our Model\n",
    "With everything put in place, the last step is to train our model\n",
    " - *batch_size (currently not supported): How many datapoints to train on for each backprop/gradient descent*\n",
    " - *epochs: How many times we loop over the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(478, 1, 7)\n",
      "(1, 7)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3346 into shape (478,1,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30599/44891558.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#X_train = np.stack([record.reshape(1,-1) for record in X_train])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3346 into shape (478,1,1)"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[0].shape)\n",
    "#X_train = np.stack([record.reshape(1,-1) for record in X_train])\n",
    "X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1])\n",
    "print(X_train.shape)\n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100   error=0.623918\n",
      "epoch 11/100   error=0.435275\n",
      "epoch 21/100   error=0.411720\n",
      "epoch 31/100   error=0.401982\n",
      "epoch 41/100   error=0.394857\n",
      "epoch 51/100   error=0.390294\n",
      "epoch 61/100   error=0.382705\n",
      "epoch 71/100   error=0.389418\n",
      "epoch 81/100   error=0.391416\n",
      "epoch 91/100   error=0.389163\n"
     ]
    }
   ],
   "source": [
    "network.fit(X_train, y_train, epochs=100, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a streamlined representation of our network\n",
    "\n",
    "```python\n",
    "network = GWUNetwork()\n",
    "network.add(Dense(14, True, activation='relu', input_size=7))\n",
    "network.add(Dense(1, True, activation='sigmoid'))\n",
    "network.compile(loss='log_loss', lr=0.001)\n",
    "network.fit(X_train, y_train, epochs=100, batch_size=20)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper Look at Training\n",
    "```python\n",
    "    def fit(self, x_train, y_train, batch_size, epochs):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j].reshape(1, -1)\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, self.learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            if i % 10 == 0:\n",
    "                err /= samples\n",
    "                print('epoch %d/%d   error=%f' % (i + 1, epochs, err))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.08301823]]), array([[0.82756661]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.predict(X_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.layers[0].weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19988928, -0.19867889,  0.40365918, -0.04466178,  0.36097635,\n",
       "         0.52146408, -0.95912783,  0.11570432,  0.06624123,  0.18749753,\n",
       "         0.45480326,  0.4796858 , -0.08597312, -0.1973516 , -1.10959328,\n",
       "         0.00513415],\n",
       "       [-0.08526921, -0.04602341, -0.53379116, -0.40084962, -0.15009928,\n",
       "         0.21694768, -0.69326546, -0.07342725,  0.067196  ,  0.34496763,\n",
       "        -0.16189939, -0.11395936,  0.47138062,  1.02167249, -0.75463628,\n",
       "        -0.17937106],\n",
       "       [ 0.03917447,  0.12356748, -0.15450242,  0.07116774, -0.06888449,\n",
       "         0.18867709, -0.80778195,  0.12348489, -0.31324665,  0.15677182,\n",
       "         0.35003437,  0.24012905,  0.26424629,  0.32659497,  0.30654192,\n",
       "        -0.03321727],\n",
       "       [ 0.35623195,  0.37366246, -0.16969693, -0.15839175,  0.16319463,\n",
       "        -0.43418166, -0.18652879,  0.17713857, -0.16295576,  0.03891437,\n",
       "         0.49707429,  0.03858724, -0.33896115, -0.3396342 ,  0.13102934,\n",
       "        -0.257091  ],\n",
       "       [-0.38067756,  0.24913177, -0.18095356,  0.15965165, -0.13028062,\n",
       "         0.13433646,  0.44619884,  0.61759254, -0.02078092, -0.27758555,\n",
       "         0.4716627 , -0.05742672,  0.32743089, -0.13801312, -0.11328073,\n",
       "         0.16384625],\n",
       "       [ 0.00234209, -0.01121233,  0.10271798,  0.04082102,  0.20065595,\n",
       "         0.37795209, -0.19921181,  0.19738936, -0.20103763,  0.30758614,\n",
       "         0.10656365, -0.14390665, -0.09183405,  0.41955039, -0.01644434,\n",
       "        -0.10396742],\n",
       "       [-0.34727544, -0.52603288, -0.18003335,  0.07703202,  0.14105343,\n",
       "        -0.00551346,  0.13188709, -0.2419604 ,  0.16842427, -0.00378467,\n",
       "         0.17194888,  0.16814289,  0.17143805,  0.20357681,  0.06907971,\n",
       "        -0.22239146]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.layers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2741055 , -0.31594925,  0.22108848, -0.42694962,  0.01050662,\n",
       "         0.50516216, -1.19725139, -0.40811571,  0.36027896,  0.30101866,\n",
       "        -0.49110829,  0.29038851,  0.18921216,  0.36938554, -1.72211045,\n",
       "         0.11841268]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].reshape(1, -1).dot(network.layers[0].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 0, 1, 0, 1, 0, 0]\n",
      "[0, 1, 1, 1, 0, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "d_round = lambda x: 1 if x >= 0.5 else 0\n",
    "predictions = [d_round(x[0][0]) for x in network.predict(X_test[:10])]\n",
    "actual = [y for y in y_test[:10].reshape(-1)]\n",
    "\n",
    "print(predictions)\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Class:\n",
    "Determine the accuracy and loss of the model using our holdout testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'list' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30599/3576087470.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_30599/3576087470.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Joel's answer (I didn't copy it right)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mthresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'list' and 'float'"
     ]
    }
   ],
   "source": [
    "# Joel's answer (I didn't copy it right)\n",
    "thresh = lambda x: 0 if x < 0.5 else 1\n",
    "\n",
    "test_size = y_test.shape[0]\n",
    "correct = 0\n",
    "for ix in range(test_size):\n",
    "    pred = thresh(network.predict(X_test[ix].reshape(-1,7)[0]))\n",
    "    if pred == y_test[1]:\n",
    "        correct +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 79.66101694915254\n"
     ]
    }
   ],
   "source": [
    "# Space for work - this matched with Joel's answer\n",
    "raw_predictions = network.predict(X_test)\n",
    "predictions = [d_round(x[0][0]) for x in raw_predictions]\n",
    "actual = [y for y in y_test.reshape(-1)]\n",
    "count = 0\n",
    "for p,a in zip(predictions,actual):\n",
    "    if p == a:\n",
    "        count += 1\n",
    "print(\"accuracy: \" + str(100 * count/len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.1481354159845752\n"
     ]
    }
   ],
   "source": [
    "# My answer (def wrong)\n",
    "mse = 0\n",
    "for a,r in zip(actual,raw_predictions):\n",
    "    mse += (a-r)**2\n",
    "mse /= len(raw_predictions)\n",
    "print(\"loss: \" + str(mse[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5018783489366799"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joel's answer\n",
    "preds = np.array(network.predict(X_test)).reshape(-1,1)\n",
    "network.loss(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Class:\n",
    "Use the GWU Network to solve one of our earlier problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
